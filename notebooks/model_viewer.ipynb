{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightningModules import AutoEncoder, FUCCIDataModule, ReconstructionVisualization\n",
    "from kornia import tensor_to_image\n",
    "from microfilm.colorify import multichannel_to_rgb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCCI_reference_VAE_2023_16_05_06_31  FUCCI_reference_VAE_2023_17_05_02_42\n",
      "FUCCI_reference_VAE_2023_16_05_06_32  FUCCI_reference_VAE_2023_17_05_02_59\n",
      "FUCCI_reference_VAE_2023_16_05_06_38  FUCCI_reference_VAE_2023_17_05_03_10\n",
      "FUCCI_reference_VAE_2023_16_05_06_39  FUCCI_reference_VAE_2023_17_05_03_11\n",
      "FUCCI_reference_VAE_2023_16_05_06_41  FUCCI_reference_VAE_2023_17_05_04_32\n",
      "FUCCI_reference_VAE_2023_16_05_06_42  FUCCI_reference_VAE_2023_17_05_04_33\n",
      "FUCCI_reference_VAE_2023_16_05_07_19  FUCCI_reference_VAE_2023_17_05_05_21\n",
      "FUCCI_reference_VAE_2023_16_05_07_22  FUCCI_reference_VAE_2023_17_05_05_22\n",
      "FUCCI_reference_VAE_2023_16_05_08_39  FUCCI_reference_VAE_2023_17_05_19_22\n",
      "FUCCI_reference_VAE_2023_16_05_08_54  FUCCI_reference_VAE_2023_17_05_19_28\n",
      "FUCCI_reference_VAE_2023_16_05_08_55  FUCCI_reference_VAE_2023_17_05_19_29\n",
      "FUCCI_reference_VAE_2023_16_05_09_01  FUCCI_reference_VAE_2023_17_05_19_30\n",
      "FUCCI_reference_VAE_2023_16_05_09_02  FUCCI_reference_VAE_2023_17_05_19_32\n",
      "FUCCI_reference_VAE_2023_16_05_10_26  FUCCI_reference_VAE_2023_17_05_19_33\n",
      "FUCCI_reference_VAE_2023_16_05_10_27  FUCCI_reference_VAE_2023_17_05_19_36\n",
      "FUCCI_reference_VAE_2023_16_05_20_52  FUCCI_reference_VAE_2023_17_05_20_33\n",
      "FUCCI_reference_VAE_2023_16_05_20_55  FUCCI_reference_VAE_2023_17_05_21_17\n",
      "FUCCI_reference_VAE_2023_16_05_21_08  FUCCI_reference_VAE_2023_17_05_21_56\n",
      "FUCCI_reference_VAE_2023_16_05_21_18  FUCCI_reference_VAE_2023_17_05_21_57\n",
      "FUCCI_reference_VAE_2023_16_05_21_25  FUCCI_reference_VAE_2023_17_05_22_30\n",
      "FUCCI_reference_VAE_2023_16_05_21_34  FUCCI_reference_VAE_2023_17_05_22_31\n",
      "FUCCI_reference_VAE_2023_16_05_22_04  FUCCI_reference_VAE_2023_17_05_23_19\n",
      "FUCCI_reference_VAE_2023_16_05_22_53  FUCCI_reference_VAE_2023_17_05_23_20\n",
      "FUCCI_reference_VAE_2023_16_05_23_52  FUCCI_reference_VAE_2023_17_05_23_44\n",
      "FUCCI_reference_VAE_2023_16_05_23_53  FUCCI_reference_VAE_2023_17_05_23_53\n",
      "FUCCI_reference_VAE_2023_16_05_23_54  FUCCI_reference_VAE_2023_17_05_23_54\n",
      "FUCCI_reference_VAE_2023_16_05_23_59  FUCCI_reference_VAE_2023_17_05_23_55\n",
      "FUCCI_reference_VAE_2023_17_05_00_16  FUCCI_reference_VAE_2023_18_05_00_53\n",
      "FUCCI_reference_VAE_2023_17_05_00_17  FUCCI_reference_VAE_2023_18_05_01_31\n",
      "FUCCI_reference_VAE_2023_17_05_02_00  FUCCI_reference_VAE_2023_18_05_02_33\n",
      "FUCCI_reference_VAE_2023_17_05_02_01  FUCCI_reference_VAE_2023_18_05_07_38\n",
      "FUCCI_reference_VAE_2023_17_05_02_16  lightning_logs\n",
      "FUCCI_reference_VAE_2023_17_05_02_17  logs\n",
      "FUCCI_reference_VAE_2023_17_05_02_41  wandb_logs\n"
     ]
    }
   ],
   "source": [
    "FUCCI_PATH = '/home/ishang/implicit-cell-model/FUCCI-dev-data'\n",
    "LOGS = '/data/ishang/fucci_vae'\n",
    "!ls {LOGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=99-Val_loss=0.00.ckpt'\n"
     ]
    }
   ],
   "source": [
    "model_folder = 'FUCCI_reference_VAE_2023_16_05_10_26'\n",
    "!ls {LOGS}/{model_folder}/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'epoch=99-Val_loss=0.00.ckpt'\n",
    "model_path = Path(LOGS) / model_folder / \"lightning_logs\" / checkpoint\n",
    "res = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ckpt['state_dict'].keys())\n",
    "# print(ckpt['state_dict'].keys())\n",
    "# ckpt['state_dict']['decoder.layers.0.0.weight'].shape\n",
    "# for k, v in ckpt['state_dict'].items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mishangaur\u001b[0m (\u001b[33mlundberg-lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ishang/implicit-cell-model/wandb/run-20230518_081351-ywfnhbn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lundberg-lab/implicit-cell-model/runs/ywfnhbn4' target=\"_blank\">prime-pond-1</a></strong> to <a href='https://wandb.ai/lundberg-lab/implicit-cell-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lundberg-lab/implicit-cell-model' target=\"_blank\">https://wandb.ai/lundberg-lab/implicit-cell-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lundberg-lab/implicit-cell-model/runs/ywfnhbn4' target=\"_blank\">https://wandb.ai/lundberg-lab/implicit-cell-model/runs/ywfnhbn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-4pyjiycg:v0, 4657.23MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:12.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./artifacts/model-4pyjiycg:v0\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('lundberg-lab/FUCCI_reference_VAE/model-4pyjiycg:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "print(artifact_dir)\n",
    "print(type(artifact_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/model-4pyjiycg:v0/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# !ls {artifact_dir}\n",
    "model_path = Path(artifact_dir) / \"model.ckpt\"\n",
    "print(model_path)\n",
    "model = torch.load(model_path)\n",
    "# model = AutoEncoder.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "<class 'FUCCIDataset.ReferenceChannelDatasetInMemory'>\n"
     ]
    }
   ],
   "source": [
    "dm = FUCCIDataModule(\n",
    "    data_dir=FUCCI_PATH,\n",
    "    dataset=\"reference\",\n",
    "    imsize=res,\n",
    "    split=(0.8, 0.1, 0.1),\n",
    "    batch_size=32,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "print(len(dm.dataset))\n",
    "print(type(dm.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dm.dataset[:3].shape)\n",
    "x = dm.dataset[:3]\n",
    "x_hat = model(x)\n",
    "print(x.shape)\n",
    "print(x_hat.shape)\n",
    "# x_hat[:, 0] *= 10\n",
    "# x_hat = torch.ones_like(x) * torch.max(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 2, 256, 256] at entry 0 and [48, 2, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grid \u001b[39m=\u001b[39m ReconstructionVisualization\u001b[39m.\u001b[39;49mmake_reconstruction_grid(x, x_hat)\n\u001b[1;32m      3\u001b[0m shape \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n",
      "File \u001b[0;32m~/implicit-cell-model/LightningModules.py:228\u001b[0m, in \u001b[0;36mReconstructionVisualization.make_reconstruction_grid\u001b[0;34m(input_imgs, reconst_imgs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_reconstruction_grid\u001b[39m(input_imgs, reconst_imgs):\n\u001b[0;32m--> 228\u001b[0m     imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([input_imgs, reconst_imgs], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mflatten(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    229\u001b[0m     grid \u001b[39m=\u001b[39m make_grid(imgs, nrow\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    230\u001b[0m     \u001b[39mreturn\u001b[39;00m grid\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 2, 256, 256] at entry 0 and [48, 2, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "x = dm.dataset[:3]\n",
    "grid = ReconstructionVisualization.make_reconstruction_grid(x, x)\n",
    "\n",
    "shape = grid.shape\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        y_st, y_end = i * shape[1] // 3, (i + 1) * shape[1] // 3\n",
    "        x_st, x_end = j * shape[2] // 2, (j + 1) * shape[2] // 2\n",
    "        for c in range(2):\n",
    "            print(torch.min(grid[c, y_st:y_end, x_st:x_end]), torch.max(grid[c, y_st:y_end, x_st:x_end]))\n",
    "    print()\n",
    "\n",
    "print(grid.shape)\n",
    "# img = np.moveaxis(tensor_to_image(grid), -1, 0)\n",
    "img = grid.cpu().detach().numpy()\n",
    "print(img.shape)\n",
    "for i in range(2):\n",
    "    image_composite, _, _, _= multichannel_to_rgb(img[i], cmaps=[dm.dataset.channel_colors()[i]])\n",
    "    image_composite = (255 * image_composite).astype(np.uint8)\n",
    "    image_composite = Image.fromarray(image_composite)\n",
    "    image_composite.save(f'pred_grid_{res}_{i}_composite.png')\n",
    "image_composite, _, _, _= multichannel_to_rgb(img, cmaps=dm.dataset.channel_colors())\n",
    "image_composite = (255 * image_composite).astype(np.uint8)\n",
    "image_composite = Image.fromarray(image_composite)\n",
    "image_composite.save(f'pred_grid_{res}_composite.png')\n",
    "\n",
    "data_grid = ReconstructionVisualization.make_reconstruction_grid(x, x)\n",
    "print(data_grid.shape)\n",
    "data_img = np.moveaxis(tensor_to_image(data_grid), -1, 0)\n",
    "print(data_img.shape)\n",
    "data_composite, _, _, _= multichannel_to_rgb(data_img, cmaps=dm.dataset.channel_colors())\n",
    "data_composite = (255 * data_composite).astype(np.uint8)\n",
    "data_composite = Image.fromarray(data_composite)\n",
    "data_composite.save(f'data_grid_{res}_composite.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
