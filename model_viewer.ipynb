{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightningModules import AutoEncoder, FUCCIDataModule, ReconstructionVisualization\n",
    "from kornia import tensor_to_image\n",
    "from microfilm.colorify import multichannel_to_rgb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCCI_reference_VAE\t\t      FUCCI_reference_VAE_28_07_05_14_2023\n",
      "FUCCI_reference_VAE_14_05_05_14_2023  FUCCI_reference_VAE_33_04_05_14_2023\n",
      "FUCCI_reference_VAE_2023_16_05_04_37  FUCCI_reference_VAE_54_07_05_13_2023\n"
     ]
    }
   ],
   "source": [
    "FUCCI_PATH = '/home/ishang/cross-modal-autoencoders/FUCCI-dev-data'\n",
    "LIGHTNING_LOGS = '/data/ishang/fucci_vae/lightning_logs'\n",
    "!ls {LIGHTNING_LOGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=02-Val_loss=0.00.ckpt'\n"
     ]
    }
   ],
   "source": [
    "model_folder = 'FUCCI_reference_VAE_2023_16_05_04_37'\n",
    "!ls {LIGHTNING_LOGS}/{model_folder}\n",
    "checkpoint = 'epoch=02-Val_loss=0.00.ckpt'\n",
    "model_path = Path(LIGHTNING_LOGS) / model_folder / checkpoint\n",
    "res = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path)\n\u001b[0;32m----> 3\u001b[0m encoder \u001b[39m=\u001b[39m ckpt[\u001b[39m'\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mencoder\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      4\u001b[0m decoder \u001b[39m=\u001b[39m ckpt[\u001b[39m'\u001b[39m\u001b[39mdecoder\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m AutoEncoder(encoder, decoder)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt = torch.load(model_path)\n",
    "encoder = ckpt['state_dict']['encoder']\n",
    "decoder = ckpt['decoder']\n",
    "model = AutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AutoEncoder.__init__() missing 2 required positional arguments: 'encoder' and 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m AutoEncoder\u001b[39m.\u001b[39;49mload_from_checkpoint(model_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/implicit/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1531\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1453\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1459\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1460\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1532\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1533\u001b[0m         checkpoint_path,\n\u001b[1;32m   1534\u001b[0m         map_location,\n\u001b[1;32m   1535\u001b[0m         hparams_file,\n\u001b[1;32m   1536\u001b[0m         strict,\n\u001b[1;32m   1537\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1538\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/implicit/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:90\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39m, checkpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39;49m, checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/implicit/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:136\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cls_spec\u001b[39m.\u001b[39mvarkw:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     _cls_kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _cls_kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 136\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_cls_kwargs)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m    139\u001b[0m     \u001b[39m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     obj\u001b[39m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "\u001b[0;31mTypeError\u001b[0m: AutoEncoder.__init__() missing 2 required positional arguments: 'encoder' and 'decoder'"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FUCCIDataModule(\n",
    "    data_dir=FUCCI_PATH,\n",
    "    dataset=\"reference\",\n",
    "    imsize=res,\n",
    "    split=(0.8, 0.1, 0.1),\n",
    "    batch_size=8,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1024, 1024])\n",
      "torch.Size([2, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(dm.dataset[:3].shape)\n",
    "print(dm.dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3080, 2054])\n",
      "(2, 3080, 2054)\n"
     ]
    }
   ],
   "source": [
    "grid = ReconstructionVisualization.make_reconstruction_grid(dm.dataset[:3], dm.dataset[:3])\n",
    "print(grid.shape)\n",
    "img = np.moveaxis(tensor_to_image(grid), -1, 0)\n",
    "print(img.shape)\n",
    "image_composite, _, _, _= multichannel_to_rgb(img, cmaps=dm.dataset.channel_colors())\n",
    "image_composite = (255 * image_composite).astype(np.uint8)\n",
    "image_composite = Image.fromarray(image_composite)\n",
    "image_composite.save(f'test_grid_{res}_composite.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
