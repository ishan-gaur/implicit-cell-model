{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightningModules import AutoEncoder, FUCCIDataModule, ReconstructionVisualization\n",
    "from kornia import tensor_to_image\n",
    "from microfilm.colorify import multichannel_to_rgb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCCI_reference_VAE_2023_16_05_06_31  FUCCI_reference_VAE_2023_16_05_08_54\n",
      "FUCCI_reference_VAE_2023_16_05_06_32  FUCCI_reference_VAE_2023_16_05_08_55\n",
      "FUCCI_reference_VAE_2023_16_05_06_38  FUCCI_reference_VAE_2023_16_05_09_01\n",
      "FUCCI_reference_VAE_2023_16_05_06_39  FUCCI_reference_VAE_2023_16_05_09_02\n",
      "FUCCI_reference_VAE_2023_16_05_06_41  FUCCI_reference_VAE_2023_16_05_10_26\n",
      "FUCCI_reference_VAE_2023_16_05_06_42  FUCCI_reference_VAE_2023_16_05_10_27\n",
      "FUCCI_reference_VAE_2023_16_05_07_19  lightning_logs\n",
      "FUCCI_reference_VAE_2023_16_05_07_22  logs\n",
      "FUCCI_reference_VAE_2023_16_05_08_39  wandb_logs\n"
     ]
    }
   ],
   "source": [
    "FUCCI_PATH = '/home/ishang/cross-modal-autoencoders/FUCCI-dev-data'\n",
    "LOGS = '/data/ishang/fucci_vae'\n",
    "!ls {LOGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=88-Val_loss=0.00.ckpt'\n"
     ]
    }
   ],
   "source": [
    "model_folder = 'FUCCI_reference_VAE_2023_16_05_10_26'\n",
    "!ls {LOGS}/{model_folder}/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'epoch=88-Val_loss=0.00.ckpt'\n",
    "model_path = Path(LOGS) / model_folder / \"lightning_logs\" / checkpoint\n",
    "res = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ckpt['state_dict'].keys())\n",
    "# print(ckpt['state_dict'].keys())\n",
    "# ckpt['state_dict']['decoder.layers.0.0.weight'].shape\n",
    "# for k, v in ckpt['state_dict'].items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FUCCIDataModule(\n",
    "    data_dir=FUCCI_PATH,\n",
    "    dataset=\"reference\",\n",
    "    imsize=res,\n",
    "    split=(0.8, 0.1, 0.1),\n",
    "    batch_size=8,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 64, 64])\n",
      "torch.Size([3, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# print(dm.dataset[:3].shape)\n",
    "x = dm.dataset[:3]\n",
    "x_hat = model(x)\n",
    "print(x.shape)\n",
    "print(x_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.5632)\n",
      "tensor(0.) tensor(1.)\n",
      "tensor(0.) tensor(0.4199)\n",
      "tensor(0.) tensor(0.4124)\n",
      "\n",
      "tensor(0.) tensor(0.5236)\n",
      "tensor(0.) tensor(0.6551)\n",
      "tensor(0.) tensor(0.4588)\n",
      "tensor(0.) tensor(0.4510)\n",
      "\n",
      "tensor(0.) tensor(0.8883)\n",
      "tensor(0.) tensor(0.8469)\n",
      "tensor(0.) tensor(0.6327)\n",
      "tensor(0.) tensor(0.6214)\n",
      "\n",
      "torch.Size([2, 200, 134])\n",
      "(2, 200, 134)\n",
      "torch.Size([2, 200, 134])\n",
      "(2, 200, 134)\n"
     ]
    }
   ],
   "source": [
    "grid = ReconstructionVisualization.make_reconstruction_grid(x, x_hat)\n",
    "\n",
    "shape = grid.shape\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        y_st, y_end = i * shape[1] // 3, (i + 1) * shape[1] // 3\n",
    "        x_st, x_end = j * shape[2] // 2, (j + 1) * shape[2] // 2\n",
    "        for c in range(2):\n",
    "            print(torch.min(grid[c, y_st:y_end, x_st:x_end]), torch.max(grid[c, y_st:y_end, x_st:x_end]))\n",
    "    print()\n",
    "\n",
    "print(grid.shape)\n",
    "# img = np.moveaxis(tensor_to_image(grid), -1, 0)\n",
    "img = grid.cpu().detach().numpy()\n",
    "print(img.shape)\n",
    "for i in range(2):\n",
    "    image_composite, _, _, _= multichannel_to_rgb(img[i], cmaps=[dm.dataset.channel_colors()[i]])\n",
    "    image_composite = (255 * image_composite).astype(np.uint8)\n",
    "    image_composite = Image.fromarray(image_composite)\n",
    "    image_composite.save(f'pred_grid_{res}_{i}_composite.png')\n",
    "image_composite, _, _, _= multichannel_to_rgb(img, cmaps=dm.dataset.channel_colors())\n",
    "image_composite = (255 * image_composite).astype(np.uint8)\n",
    "image_composite = Image.fromarray(image_composite)\n",
    "image_composite.save(f'pred_grid_{res}_composite.png')\n",
    "\n",
    "data_grid = ReconstructionVisualization.make_reconstruction_grid(x, x)\n",
    "print(data_grid.shape)\n",
    "data_img = np.moveaxis(tensor_to_image(data_grid), -1, 0)\n",
    "print(data_img.shape)\n",
    "data_composite, _, _, _= multichannel_to_rgb(data_img, cmaps=dm.dataset.channel_colors())\n",
    "data_composite = (255 * data_composite).astype(np.uint8)\n",
    "data_composite = Image.fromarray(data_composite)\n",
    "data_composite.save(f'data_grid_{res}_composite.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
