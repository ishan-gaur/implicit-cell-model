{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightningModules import AutoEncoder, FUCCIDataModule, ReconstructionVisualization\n",
    "from kornia import tensor_to_image\n",
    "from microfilm.colorify import multichannel_to_rgb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCCI_reference_VAE_2023_16_05_06_31  FUCCI_reference_VAE_2023_16_05_22_04\n",
      "FUCCI_reference_VAE_2023_16_05_06_32  FUCCI_reference_VAE_2023_16_05_22_53\n",
      "FUCCI_reference_VAE_2023_16_05_06_38  FUCCI_reference_VAE_2023_16_05_23_52\n",
      "FUCCI_reference_VAE_2023_16_05_06_39  FUCCI_reference_VAE_2023_16_05_23_53\n",
      "FUCCI_reference_VAE_2023_16_05_06_41  FUCCI_reference_VAE_2023_16_05_23_54\n",
      "FUCCI_reference_VAE_2023_16_05_06_42  FUCCI_reference_VAE_2023_16_05_23_59\n",
      "FUCCI_reference_VAE_2023_16_05_07_19  FUCCI_reference_VAE_2023_17_05_00_16\n",
      "FUCCI_reference_VAE_2023_16_05_07_22  FUCCI_reference_VAE_2023_17_05_00_17\n",
      "FUCCI_reference_VAE_2023_16_05_08_39  FUCCI_reference_VAE_2023_17_05_02_00\n",
      "FUCCI_reference_VAE_2023_16_05_08_54  FUCCI_reference_VAE_2023_17_05_02_01\n",
      "FUCCI_reference_VAE_2023_16_05_08_55  FUCCI_reference_VAE_2023_17_05_02_16\n",
      "FUCCI_reference_VAE_2023_16_05_09_01  FUCCI_reference_VAE_2023_17_05_02_17\n",
      "FUCCI_reference_VAE_2023_16_05_09_02  FUCCI_reference_VAE_2023_17_05_02_41\n",
      "FUCCI_reference_VAE_2023_16_05_10_26  FUCCI_reference_VAE_2023_17_05_02_42\n",
      "FUCCI_reference_VAE_2023_16_05_10_27  FUCCI_reference_VAE_2023_17_05_02_59\n",
      "FUCCI_reference_VAE_2023_16_05_20_52  FUCCI_reference_VAE_2023_17_05_03_10\n",
      "FUCCI_reference_VAE_2023_16_05_20_55  FUCCI_reference_VAE_2023_17_05_03_11\n",
      "FUCCI_reference_VAE_2023_16_05_21_08  lightning_logs\n",
      "FUCCI_reference_VAE_2023_16_05_21_18  logs\n",
      "FUCCI_reference_VAE_2023_16_05_21_25  wandb_logs\n",
      "FUCCI_reference_VAE_2023_16_05_21_34\n"
     ]
    }
   ],
   "source": [
    "FUCCI_PATH = '/home/ishang/cross-modal-autoencoders/FUCCI-dev-data'\n",
    "LOGS = '/data/ishang/fucci_vae'\n",
    "!ls {LOGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=99-Val_loss=0.00.ckpt'\n"
     ]
    }
   ],
   "source": [
    "model_folder = 'FUCCI_reference_VAE_2023_16_05_10_26'\n",
    "!ls {LOGS}/{model_folder}/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'epoch=99-Val_loss=0.00.ckpt'\n",
    "model_path = Path(LOGS) / model_folder / \"lightning_logs\" / checkpoint\n",
    "res = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ckpt['state_dict'].keys())\n",
    "# print(ckpt['state_dict'].keys())\n",
    "# ckpt['state_dict']['decoder.layers.0.0.weight'].shape\n",
    "# for k, v in ckpt['state_dict'].items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "<class 'FUCCIDataset.ReferenceChannelDataset'>\n"
     ]
    }
   ],
   "source": [
    "dm = FUCCIDataModule(\n",
    "    data_dir=FUCCI_PATH,\n",
    "    dataset=\"reference\",\n",
    "    imsize=res,\n",
    "    split=(0.8, 0.1, 0.1),\n",
    "    batch_size=32,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "print(len(dm.dataset))\n",
    "print(type(dm.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 64, 64])\n",
      "torch.Size([3, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# print(dm.dataset[:3].shape)\n",
    "x = dm.dataset[:3]\n",
    "x_hat = model(x)\n",
    "print(x.shape)\n",
    "print(x_hat.shape)\n",
    "x_hat[:, 0] *= 10\n",
    "# x_hat = torch.ones_like(x) * torch.max(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.0785)\n",
      "tensor(0.) tensor(0.1387)\n",
      "tensor(0.) tensor(0.5694)\n",
      "tensor(0.) tensor(0.0562)\n",
      "\n",
      "tensor(0.) tensor(0.0730)\n",
      "tensor(0.) tensor(0.0912)\n",
      "tensor(0.) tensor(0.7912)\n",
      "tensor(0.) tensor(0.0780)\n",
      "\n",
      "tensor(0.) tensor(0.1233)\n",
      "tensor(0.) tensor(0.1176)\n",
      "tensor(0.) tensor(1.)\n",
      "tensor(0.) tensor(0.0982)\n",
      "\n",
      "torch.Size([2, 200, 134])\n",
      "(2, 200, 134)\n",
      "torch.Size([2, 200, 134])\n",
      "(2, 200, 134)\n"
     ]
    }
   ],
   "source": [
    "grid = ReconstructionVisualization.make_reconstruction_grid(x, x_hat)\n",
    "\n",
    "shape = grid.shape\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        y_st, y_end = i * shape[1] // 3, (i + 1) * shape[1] // 3\n",
    "        x_st, x_end = j * shape[2] // 2, (j + 1) * shape[2] // 2\n",
    "        for c in range(2):\n",
    "            print(torch.min(grid[c, y_st:y_end, x_st:x_end]), torch.max(grid[c, y_st:y_end, x_st:x_end]))\n",
    "    print()\n",
    "\n",
    "print(grid.shape)\n",
    "# img = np.moveaxis(tensor_to_image(grid), -1, 0)\n",
    "img = grid.cpu().detach().numpy()\n",
    "print(img.shape)\n",
    "for i in range(2):\n",
    "    image_composite, _, _, _= multichannel_to_rgb(img[i], cmaps=[dm.dataset.channel_colors()[i]])\n",
    "    image_composite = (255 * image_composite).astype(np.uint8)\n",
    "    image_composite = Image.fromarray(image_composite)\n",
    "    image_composite.save(f'pred_grid_{res}_{i}_composite.png')\n",
    "image_composite, _, _, _= multichannel_to_rgb(img, cmaps=dm.dataset.channel_colors())\n",
    "image_composite = (255 * image_composite).astype(np.uint8)\n",
    "image_composite = Image.fromarray(image_composite)\n",
    "image_composite.save(f'pred_grid_{res}_composite.png')\n",
    "\n",
    "data_grid = ReconstructionVisualization.make_reconstruction_grid(x, x)\n",
    "print(data_grid.shape)\n",
    "data_img = np.moveaxis(tensor_to_image(data_grid), -1, 0)\n",
    "print(data_img.shape)\n",
    "data_composite, _, _, _= multichannel_to_rgb(data_img, cmaps=dm.dataset.channel_colors())\n",
    "data_composite = (255 * data_composite).astype(np.uint8)\n",
    "data_composite = Image.fromarray(data_composite)\n",
    "data_composite.save(f'data_grid_{res}_composite.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
